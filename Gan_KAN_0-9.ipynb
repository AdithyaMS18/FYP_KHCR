{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201e42d5-9dd1-4efe-a029-45404f4586c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e48207fc-34f2-4d6c-abb2-21094b1622b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator model\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding='same', input_shape=img_shape),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947f977b-9e2d-4650-acf6-7059b69b689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, z_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(7*7*512, input_dim=z_shape),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Reshape((7, 7, 512)),\n",
    "            tf.keras.layers.UpSampling2D(),\n",
    "            tf.keras.layers.Conv2D(256, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.UpSampling2D(),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2D(1, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.Activation('tanh')\n",
    "        ])\n",
    "\n",
    "    def call(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197182a2-e9b6-4e64-bd19-00958d40b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN class\n",
    "class DCGAN:\n",
    "    def __init__(self, img_shape, epochs=50000, lr_gen=0.0001, lr_disc=0.0001, z_shape=100, batch_size=64, epochs_for_sample=5000):\n",
    "        self.img_shape = img_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.z_shape = z_shape\n",
    "        self.epochs_for_sample = epochs_for_sample\n",
    "        self.generator = Generator(self.z_shape)\n",
    "        self.discriminator = Discriminator(self.img_shape)\n",
    "\n",
    "        # Optimizers\n",
    "        self.gen_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_gen)\n",
    "        self.disc_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_disc)\n",
    "\n",
    "    def train(self, X_train):\n",
    "        for epoch in range(self.epochs):\n",
    "            idx = np.random.randint(0, len(X_train), self.batch_size)\n",
    "            batch_X = X_train[idx]\n",
    "            batch_Z = np.random.uniform(-1, 1, (self.batch_size, self.z_shape))\n",
    "\n",
    "            # Train Discriminator\n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                disc_logits_real = self.discriminator(batch_X)\n",
    "                disc_logits_fake = self.discriminator(self.generator(batch_Z))\n",
    "                disc_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.ones_like(disc_logits_real), logits=disc_logits_real)) + \\\n",
    "                             tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.zeros_like(disc_logits_fake), logits=disc_logits_fake))\n",
    "\n",
    "            grads_disc = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "            self.disc_optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_variables))\n",
    "\n",
    "            # Train Generator\n",
    "            with tf.GradientTape() as gen_tape:\n",
    "                gen_imgs = self.generator(batch_Z)\n",
    "                disc_logits_fake = self.discriminator(gen_imgs)\n",
    "                gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.ones_like(disc_logits_fake), logits=disc_logits_fake))\n",
    "\n",
    "            grads_gen = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "            self.gen_optimizer.apply_gradients(zip(grads_gen, self.generator.trainable_variables))\n",
    "\n",
    "            # Generate samples every 5000 epochs\n",
    "            if epoch % self.epochs_for_sample == 0:\n",
    "                self.generate_sample(epoch)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch: {epoch}, Discriminator Loss: {disc_loss.numpy()}, Generator Loss: {gen_loss.numpy()}\")\n",
    "\n",
    "    def generate_sample(self, epoch):\n",
    "        z = np.random.uniform(-1, 1, (self.batch_size, self.z_shape))\n",
    "        imgs = self.generator(z)\n",
    "        imgs = (imgs.numpy() * 0.5 + 0.5)  # Scale back to [0, 1]\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            img = imgs[i, :, :, 0]  # Select the individual image\n",
    "            label = np.random.randint(0, 10)  # Replace with the actual label you want to associate\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Label: {label}')  # Show label on the image\n",
    "            \n",
    "            # Save individual images with label in the filename\n",
    "            plt.savefig(f\"samples/{epoch}_img_{i}_label_{label}.png\", bbox_inches='tight')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2cafe3-f0b3-4b37-8d34-0b8e7c1e0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "\n",
    "    X_train = train_df.iloc[:, 1:].values.astype(np.float32)  # Exclude the label column\n",
    "    y_train = train_df.iloc[:, 0].values  # Labels\n",
    "\n",
    "    X_test = test_df.iloc[:, 1:].values.astype(np.float32)  # Exclude the label column\n",
    "    y_test = test_df.iloc[:, 0].values  # Labels\n",
    "\n",
    "    # Preprocess data: scale to [-1, 1] and reshape\n",
    "    X_train = (X_train / 255.0) * 2 - 1  # Scale to [-1, 1]\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # Reshape to (samples, 28, 28, 1)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e76e89c-5a33-4a3f-b3f6-b0e9020e16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\msadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\msadi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Discriminator Loss: 1.3481866121292114, Generator Loss: 0.6917556524276733\n",
      "Epoch: 100, Discriminator Loss: 0.0016148430295288563, Generator Loss: 6.676881790161133\n",
      "Epoch: 200, Discriminator Loss: 0.0005105931777507067, Generator Loss: 8.006937980651855\n",
      "Epoch: 300, Discriminator Loss: 0.4793214201927185, Generator Loss: 1.011662483215332\n",
      "Epoch: 400, Discriminator Loss: 0.8543341159820557, Generator Loss: 2.0203163623809814\n",
      "Epoch: 500, Discriminator Loss: 0.008884875103831291, Generator Loss: 5.362359046936035\n",
      "Epoch: 600, Discriminator Loss: 0.003193259472027421, Generator Loss: 6.215540409088135\n",
      "Epoch: 700, Discriminator Loss: 0.0025000236928462982, Generator Loss: 6.625896453857422\n",
      "Epoch: 800, Discriminator Loss: 0.002011875854805112, Generator Loss: 7.139761924743652\n",
      "Epoch: 900, Discriminator Loss: 0.0012219077907502651, Generator Loss: 7.6170806884765625\n",
      "Epoch: 1000, Discriminator Loss: 1.1001336574554443, Generator Loss: 1.2697343826293945\n",
      "Epoch: 1100, Discriminator Loss: 0.5896309018135071, Generator Loss: 1.6113696098327637\n",
      "Epoch: 1200, Discriminator Loss: 0.506797194480896, Generator Loss: 2.755596876144409\n",
      "Epoch: 1300, Discriminator Loss: 0.95279860496521, Generator Loss: 1.869985818862915\n",
      "Epoch: 1400, Discriminator Loss: 0.3259161114692688, Generator Loss: 3.3731298446655273\n",
      "Epoch: 1500, Discriminator Loss: 0.14658178389072418, Generator Loss: 3.250326633453369\n",
      "Epoch: 1600, Discriminator Loss: 0.6787616014480591, Generator Loss: 1.6411664485931396\n",
      "Epoch: 1700, Discriminator Loss: 0.5610053539276123, Generator Loss: 2.998544692993164\n",
      "Epoch: 1800, Discriminator Loss: 0.3499052822589874, Generator Loss: 2.2355120182037354\n",
      "Epoch: 1900, Discriminator Loss: 0.6454629898071289, Generator Loss: 1.8084527254104614\n",
      "Epoch: 2000, Discriminator Loss: 0.926283597946167, Generator Loss: 1.5556237697601318\n",
      "Epoch: 2100, Discriminator Loss: 1.5039149522781372, Generator Loss: 1.9255926609039307\n",
      "Epoch: 2200, Discriminator Loss: 1.209585428237915, Generator Loss: 1.8655369281768799\n",
      "Epoch: 2300, Discriminator Loss: 0.679305374622345, Generator Loss: 1.995927333831787\n",
      "Epoch: 2400, Discriminator Loss: 0.9429032206535339, Generator Loss: 1.917741298675537\n",
      "Epoch: 2500, Discriminator Loss: 0.06431560963392258, Generator Loss: 3.9440221786499023\n",
      "Epoch: 2600, Discriminator Loss: 1.0106390714645386, Generator Loss: 1.6726720333099365\n",
      "Epoch: 2700, Discriminator Loss: 0.4287514090538025, Generator Loss: 2.537459135055542\n",
      "Epoch: 2800, Discriminator Loss: 2.424813747406006, Generator Loss: 1.3084380626678467\n",
      "Epoch: 2900, Discriminator Loss: 1.3103728294372559, Generator Loss: 1.2002075910568237\n",
      "Epoch: 3000, Discriminator Loss: 0.38366708159446716, Generator Loss: 2.9558699131011963\n",
      "Epoch: 3100, Discriminator Loss: 1.272002935409546, Generator Loss: 1.480791687965393\n",
      "Epoch: 3200, Discriminator Loss: 0.33402490615844727, Generator Loss: 2.9664807319641113\n",
      "Epoch: 3300, Discriminator Loss: 1.039970874786377, Generator Loss: 1.3412508964538574\n",
      "Epoch: 3400, Discriminator Loss: 0.6172797679901123, Generator Loss: 1.614455223083496\n",
      "Epoch: 3500, Discriminator Loss: 0.7774161696434021, Generator Loss: 1.7562425136566162\n",
      "Epoch: 3600, Discriminator Loss: 1.2244877815246582, Generator Loss: 1.6497116088867188\n",
      "Epoch: 3700, Discriminator Loss: 1.0028927326202393, Generator Loss: 1.4227426052093506\n",
      "Epoch: 3800, Discriminator Loss: 1.0041217803955078, Generator Loss: 1.1871025562286377\n",
      "Epoch: 3900, Discriminator Loss: 1.015797734260559, Generator Loss: 1.597952127456665\n",
      "Epoch: 4000, Discriminator Loss: 1.1369524002075195, Generator Loss: 1.0220168828964233\n",
      "Epoch: 4100, Discriminator Loss: 0.7651764154434204, Generator Loss: 1.7099499702453613\n",
      "Epoch: 4200, Discriminator Loss: 0.9909065961837769, Generator Loss: 1.1578843593597412\n",
      "Epoch: 4300, Discriminator Loss: 0.8436328172683716, Generator Loss: 1.4877499341964722\n",
      "Epoch: 4400, Discriminator Loss: 0.9687544107437134, Generator Loss: 1.391257882118225\n",
      "Epoch: 4500, Discriminator Loss: 0.8963167667388916, Generator Loss: 1.3710651397705078\n",
      "Epoch: 4600, Discriminator Loss: 0.9482156038284302, Generator Loss: 1.3635317087173462\n",
      "Epoch: 4700, Discriminator Loss: 1.0781199932098389, Generator Loss: 1.2377879619598389\n",
      "Epoch: 4800, Discriminator Loss: 1.0600165128707886, Generator Loss: 1.0107052326202393\n",
      "Epoch: 4900, Discriminator Loss: 0.9090859889984131, Generator Loss: 1.1690157651901245\n",
      "Epoch: 5000, Discriminator Loss: 1.0662868022918701, Generator Loss: 1.1358376741409302\n",
      "Epoch: 5100, Discriminator Loss: 1.3777127265930176, Generator Loss: 1.0289757251739502\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11380\\1176533922.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"samples\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"samples\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdcgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdcgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11380\\3087935500.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X_train)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mdisc_logits_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n\u001b[0;32m     39\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc_logits_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisc_logits_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mgrads_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# Generate samples every 5000 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1062\u001b[0m               \u001b[0moutput_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                           \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_gradients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \u001b[1;34m\"Unknown value for unconnected_gradients: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0munconnected_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[1;31m# to use the nn_ops functions, we would have to convert `padding` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m   \u001b[1;31m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m   \u001b[1;31m# in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m   return [\n\u001b[1;32m--> 584\u001b[1;33m       gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[0;32m    585\u001b[0m           \u001b[0mshape_0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2030\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dilations\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2031\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2032\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2033\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2034\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2035\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m       return conv2d_backprop_input_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    img_shape = (28, 28, 1)  # Shape of the images\n",
    "    X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "    # Create samples directory\n",
    "    if not os.path.exists(\"samples\"):\n",
    "        os.makedirs(\"samples\")\n",
    "\n",
    "    dcgan = DCGAN(img_shape)\n",
    "    dcgan.train(X_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
